

<!DOCTYPE html>
<html class="writer-html5" lang="en" data-content_root="./">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Cadre Théorique &mdash; PC-UDRL 0.1 documentation</title>
      <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b86133f3" />
      <link rel="stylesheet" type="text/css" href="_static/css/theme.css?v=e59714d7" />

  
      <script src="_static/documentation_options.js?v=2709fde1"></script>
      <script src="_static/doctools.js?v=9a2dae69"></script>
      <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
      <script async="async" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script src="_static/js/theme.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="Méthodologie Algorithmique" href="methodology.html" />
    <link rel="prev" title="Usage Guide" href="usage.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="index.html" class="icon icon-home">
            PC-UDRL
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" aria-label="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="Navigation menu">
              <p class="caption" role="heading"><span class="caption-text">Contents:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="installation.html">Installation</a></li>
<li class="toctree-l1"><a class="reference internal" href="usage.html">Usage Guide</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Cadre Théorique</a><ul>
<li class="toctree-l2"><a class="reference internal" href="#le-defi-du-rl-offline-the-optimism-trap">1. Le Défi du RL Offline : “The Optimism Trap”</a></li>
<li class="toctree-l2"><a class="reference internal" href="#upside-down-rl-fondement-de-notre-travail">2. Upside-Down RL : Fondement de Notre Travail</a></li>
<li class="toctree-l2"><a class="reference internal" href="#etat-de-l-art-les-limites-existantes">3. État de l’Art : Les Limites Existantes</a></li>
<li class="toctree-l2"><a class="reference internal" href="#pc-udrl-notre-approche">4. PC-UDRL : Notre Approche</a></li>
<li class="toctree-l2"><a class="reference internal" href="#les-trois-niveaux-de-sophistication">5. Les Trois Niveaux de Sophistication</a><ul>
<li class="toctree-l3"><a class="reference internal" href="#niveau-1-regression-quantile-approche-scalaire">Niveau 1 : Régression Quantile (Approche Scalaire)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#niveau-2-conditional-vae-approche-latente">Niveau 2 : Conditional VAE (Approche Latente)</a></li>
<li class="toctree-l3"><a class="reference internal" href="#niveau-3-score-based-diffusion-approche-manifold">Niveau 3 : Score-Based Diffusion (Approche Manifold)</a></li>
</ul>
</li>
<li class="toctree-l2"><a class="reference internal" href="#bibliographie-liens-utiles">6. Bibliographie &amp; Liens Utiles</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="methodology.html">Méthodologie Algorithmique</a></li>
<li class="toctree-l1"><a class="reference internal" href="experiments.html">Protocole Expérimental</a></li>
<li class="toctree-l1"><a class="reference internal" href="results.html">Results &amp; Benchmarks</a></li>
<li class="toctree-l1"><a class="reference internal" href="safety.html">Safety &amp; Robustness</a></li>
<li class="toctree-l1"><a class="reference internal" href="api.html">API Reference</a></li>
<li class="toctree-l1"><a class="reference internal" href="faq.html">FAQ &amp; Deep Dive</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="Mobile navigation menu" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="index.html">PC-UDRL</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="Page navigation">
  <ul class="wy-breadcrumbs">
      <li><a href="index.html" class="icon icon-home" aria-label="Home"></a></li>
      <li class="breadcrumb-item active">Cadre Théorique</li>
      <li class="wy-breadcrumbs-aside">
            <a href="_sources/theory.rst.txt" rel="nofollow"> View page source</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="cadre-theorique">
<h1>Cadre Théorique<a class="headerlink" href="#cadre-theorique" title="Link to this heading">¶</a></h1>
<nav class="contents local" id="sommaire">
<p class="topic-title">Sommaire</p>
<ul class="simple">
<li><p><a class="reference internal" href="#le-defi-du-rl-offline-the-optimism-trap" id="id2">1. Le Défi du RL Offline : “The Optimism Trap”</a></p></li>
<li><p><a class="reference internal" href="#upside-down-rl-fondement-de-notre-travail" id="id3">2. Upside-Down RL : Fondement de Notre Travail</a></p></li>
<li><p><a class="reference internal" href="#etat-de-l-art-les-limites-existantes" id="id4">3. État de l’Art : Les Limites Existantes</a></p></li>
<li><p><a class="reference internal" href="#pc-udrl-notre-approche" id="id5">4. PC-UDRL : Notre Approche</a></p></li>
<li><p><a class="reference internal" href="#les-trois-niveaux-de-sophistication" id="id6">5. Les Trois Niveaux de Sophistication</a></p>
<ul>
<li><p><a class="reference internal" href="#niveau-1-regression-quantile-approche-scalaire" id="id7">Niveau 1 : Régression Quantile (Approche Scalaire)</a></p></li>
<li><p><a class="reference internal" href="#niveau-2-conditional-vae-approche-latente" id="id8">Niveau 2 : Conditional VAE (Approche Latente)</a></p></li>
<li><p><a class="reference internal" href="#niveau-3-score-based-diffusion-approche-manifold" id="id9">Niveau 3 : Score-Based Diffusion (Approche Manifold)</a></p></li>
</ul>
</li>
<li><p><a class="reference internal" href="#bibliographie-liens-utiles" id="id10">6. Bibliographie &amp; Liens Utiles</a></p></li>
</ul>
</nav>
<section id="le-defi-du-rl-offline-the-optimism-trap">
<h2><a class="toc-backref" href="#id2" role="doc-backlink">1. Le Défi du RL Offline : “The Optimism Trap”</a><a class="headerlink" href="#le-defi-du-rl-offline-the-optimism-trap" title="Link to this heading">¶</a></h2>
<p>Le Reinforcement Learning (RL) classique repose sur l’interaction : l’agent essaie, échoue, et apprend. Mais dans des domaines critiques (médecine, robotique industrielle, conduite), l’échec n’est pas une option.</p>
<p>Le <strong>RL Offline</strong> impose d’apprendre une politique optimale <span class="math notranslate nohighlight">\(\pi^*\)</span> uniquement à partir d’un historique de données fixes <span class="math notranslate nohighlight">\(\mathcal{D}\)</span>, sans aucune interaction supplémentaire.</p>
<p>Le problème majeur est le <strong>Biais d’Optimisme</strong> face à l’inconnu (OOD - Out of Distribution).
Pour une action <span class="math notranslate nohighlight">\(a\)</span> jamais observée dans un état <span class="math notranslate nohighlight">\(s\)</span>, les algorithmes classiques (Q-Learning) ont tendance à surestimer sa valeur <span class="math notranslate nohighlight">\(Q(s,a)\)</span>.</p>
<div class="admonition danger">
<p class="admonition-title">Danger</p>
<p><strong>Le Piège :</strong> L’agent “hallucine” qu’une action dangereuse (ex: foncer dans un mur) va rapporter une récompense infinie simplement parce qu’il n’a jamais vu la conséquence négative.</p>
</div>
</section>
<section id="upside-down-rl-fondement-de-notre-travail">
<h2><a class="toc-backref" href="#id3" role="doc-backlink">2. Upside-Down RL : Fondement de Notre Travail</a><a class="headerlink" href="#upside-down-rl-fondement-de-notre-travail" title="Link to this heading">¶</a></h2>
<p>Contrairement au RL classique qui maximise une récompense, l’<strong>Upside-Down RL (UDRL)</strong> traite le RL comme un problème d’apprentissage supervisé.</p>
<p><strong>Formulation Précise :</strong></p>
<p>À l’instant <span class="math notranslate nohighlight">\(t\)</span>, l’agent observe l’état <span class="math notranslate nohighlight">\(s_t\)</span> et reçoit une commande <span class="math notranslate nohighlight">\(c = (r^*, h^*)\)</span> spécifiant le retour cible et l’horizon désiré. Il prédit une action :</p>
<div class="math notranslate nohighlight">
\[a_t \sim \pi_\theta(a_t | s_t, r^*, h^*)\]</div>
<p><strong>Entraînement (Supervised Learning) :</strong></p>
<p>Sur un dataset <span class="math notranslate nohighlight">\(\mathcal{D} = \{\tau_i\}\)</span>, pour chaque transition <span class="math notranslate nohighlight">\((s_t, a_t, r_t, ..., s_T)\)</span> :</p>
<ol class="arabic simple">
<li><p>Calculer la commande atteinte (Ground Truth) :
<span class="math notranslate nohighlight">\(r_t^{achieved} = \sum_{k=t}^T r_k\)</span> et <span class="math notranslate nohighlight">\(h_t^{remaining} = T - t\)</span>.</p></li>
<li><p>Minimiser la perte comportementale (Negative Log-Likelihood) :</p></li>
</ol>
<div class="math notranslate nohighlight">
\[\mathcal{L}(\theta) = \mathbb{E}_{(s,a,r^{achieved},h^{remaining}) \sim \mathcal{D}} \left[ || \pi_\theta(s, r^{achieved}, h^{remaining}) - a ||^2 \right]\]</div>
</section>
<section id="etat-de-l-art-les-limites-existantes">
<h2><a class="toc-backref" href="#id4" role="doc-backlink">3. État de l’Art : Les Limites Existantes</a><a class="headerlink" href="#etat-de-l-art-les-limites-existantes" title="Link to this heading">¶</a></h2>
<p>La littérature actuelle tente de résoudre le problème OOD par des contraintes punitives.</p>
<p><strong>A. Baselines Classiques</strong></p>
<table class="docutils align-default" id="id1">
<caption><span class="caption-text">Performances État de l’Art (LunarLander Medium)</span><a class="headerlink" href="#id1" title="Link to this table">¶</a></caption>
<thead>
<tr class="row-odd"><th class="head"><p>Méthode</p></th>
<th class="head"><p>Mécanisme</p></th>
<th class="head"><p>Limite</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p><strong>CQL</strong> (Kumar et al.)</p></td>
<td><p><strong>Value Regularization</strong>. Punit les Q-valeurs OOD.</p></td>
<td><p><strong>Trop Conservateur</strong>. L’agent “gèle” sur place.</p></td>
</tr>
<tr class="row-odd"><td><p><strong>TD3+BC</strong> (Fujimoto et al.)</p></td>
<td><p><strong>Policy Constraint</strong>. Force <span class="math notranslate nohighlight">\(\pi \approx \pi_\beta\)</span>.</p></td>
<td><p><strong>Plafond de Verre</strong>. Ne peut pas dépasser l’expert moyen.</p></td>
</tr>
<tr class="row-even"><td><p><strong>IQL</strong> (Kostrikov et al.)</p></td>
<td><p><strong>Expectile Regression</strong>. Ne query jamais d’actions OOD.</p></td>
<td><p>Complexe et sensible aux hyperparamètres.</p></td>
</tr>
</tbody>
</table>
<p><strong>B. La Lacune Identifiée</strong></p>
<p>Toutes ces méthodes brident <strong>l’agilité</strong> de l’agent pour assurer sa <strong>sécurité</strong>.
Notre thèse : <em>Il ne faut pas brider l’agent, il faut filtrer ses ordres.</em></p>
</section>
<section id="pc-udrl-notre-approche">
<h2><a class="toc-backref" href="#id5" role="doc-backlink">4. PC-UDRL : Notre Approche</a><a class="headerlink" href="#pc-udrl-notre-approche" title="Link to this heading">¶</a></h2>
<p><strong>Pessimistic Command UDRL</strong> propose une architecture découplée :</p>
<ol class="arabic simple">
<li><p><strong>L’Agent (Le Pilote)</strong> : Un modèle UDRL pur, agile et obéissant.</p></li>
<li><p><strong>Le Pessimiste (Le Garde-Fou)</strong> : Un modèle distinct qui apprend la <em>frontière de faisabilité</em> du monde.</p></li>
</ol>
<p><strong>L’Innovation : Pessimisme de Commande</strong>
Au lieu de modifier les poids de l’agent (comme CQL), nous modifions son <strong>entrée</strong> (input projection).</p>
<div class="math notranslate nohighlight">
\[r_{safe} = \text{Project}(r_{target} | s)\]</div>
<p>Si l’utilisateur demande l’impossible, le Pessimiste projette cette demande sur le manifold des retours réalisables.</p>
</section>
<section id="les-trois-niveaux-de-sophistication">
<h2><a class="toc-backref" href="#id6" role="doc-backlink">5. Les Trois Niveaux de Sophistication</a><a class="headerlink" href="#les-trois-niveaux-de-sophistication" title="Link to this heading">¶</a></h2>
<p>Nous avons exploré trois paradigmes pour modéliser cette “Frontière de Faisabilité”.</p>
<section id="niveau-1-regression-quantile-approche-scalaire">
<h3><a class="toc-backref" href="#id7" role="doc-backlink">Niveau 1 : Régression Quantile (Approche Scalaire)</a><a class="headerlink" href="#niveau-1-regression-quantile-approche-scalaire" title="Link to this heading">¶</a></h3>
<p><strong>Principe :</strong>
On apprend séparément les bornes de faisabilité pour le retour <span class="math notranslate nohighlight">\(r\)</span> et l’horizon <span class="math notranslate nohighlight">\(h\)</span>.</p>
<p><strong>Formulation Mathématique :</strong>
On apprend séparément les quantiles marginaux <span class="math notranslate nohighlight">\(Q_\phi^r(\tau)\)</span> et <span class="math notranslate nohighlight">\(Q_\phi^h(\tau)\)</span>.
La fonction de perte est la somme des <strong>Pinball Losses</strong> :</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{QR}(\phi) = \sum_{\tau} \mathbb{E}_{(r,h) \sim \mathcal{D}} [\rho_\tau(r - Q_\phi^r(\tau)) + \rho_\tau(h - Q_\phi^h(\tau))]\]</div>
<p>Où <span class="math notranslate nohighlight">\(\rho_\tau(u) = |u| \times (\tau - \mathbb{I}_{u &lt; 0})\)</span> est la perte asymétrique.</p>
<p><strong>Inférence (Clamping Indépendant) :</strong></p>
<div class="math notranslate nohighlight">
\[r_{safe} = \text{clip}(r_{target}, Q_\phi^r(\tau_{min}), Q_\phi^r(\tau_{max}))\]</div>
<ul class="simple">
<li><p><strong>Verdict</strong> : <strong>Trop Conservateur</strong>. Cette méthode ignore les corrélations entre <span class="math notranslate nohighlight">\(r\)</span> et <span class="math notranslate nohighlight">\(h\)</span>. Elle définit le domaine de sécurité comme un rectangle, or c’est souvent une forme plus complexe.</p></li>
</ul>
</section>
<section id="niveau-2-conditional-vae-approche-latente">
<h3><a class="toc-backref" href="#id8" role="doc-backlink">Niveau 2 : Conditional VAE (Approche Latente)</a><a class="headerlink" href="#niveau-2-conditional-vae-approche-latente" title="Link to this heading">¶</a></h3>
<p><strong>Principe :</strong>
On utilise un CVAE pour apprendre la densité jointe <span class="math notranslate nohighlight">\(p(r, h | s)\)</span> via un espace latent <span class="math notranslate nohighlight">\(z\)</span>.</p>
<p><strong>Formulation Mathématique (ELBO) :</strong></p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{CVAE} = \mathbb{E}_{(r,h,s) \sim \mathcal{D}} [ \underbrace{||(r,h) - \mu_\psi(z,s)||^2}_{\text{Reconstruction}} + \beta \underbrace{D_{KL}(q_\phi(z|r,h,s) || p(z))}_{\text{Régularisation}} ]\]</div>
<p>Où <span class="math notranslate nohighlight">\(\beta \geq 1\)</span> contrôle la structure de l’espace latent (<span class="math notranslate nohighlight">\(\beta\)</span>-VAE).</p>
<p><strong>Inférence (Optimisation Latente) :</strong>
On cherche le point latent <span class="math notranslate nohighlight">\(z_{safe}\)</span> qui minimise la distance à la commande cible, tout en restant dans la zone probable (prior gaussien) :</p>
<div class="math notranslate nohighlight">
\[z_{safe} = \arg\min_z ||\mu_\psi(z,s) - (r_{targets}, h_{targets})||^2 \quad \text{s.c.} \quad ||z||^2 \le R^2\]</div>
<ul class="simple">
<li><p><strong>Verdict</strong> : <strong>Trop Optimiste</strong>. Les VAEs souffrent du “Prior Hole Problem”. Ils assignent une probabilité non-nulle aux zones vides entre les clusters de données, menant à des commandes “chimériques”.</p></li>
</ul>
</section>
<section id="niveau-3-score-based-diffusion-approche-manifold">
<h3><a class="toc-backref" href="#id9" role="doc-backlink">Niveau 3 : Score-Based Diffusion (Approche Manifold)</a><a class="headerlink" href="#niveau-3-score-based-diffusion-approche-manifold" title="Link to this heading">¶</a></h3>
<p><strong>Principe :</strong>
On modélise le <strong>gradient</strong> de la log-densité <span class="math notranslate nohighlight">\(\nabla_x \log p(x|s)\)</span>. C’est l’approche la plus fidèle à la géométrie des données.</p>
<p><strong>Formulation Mathématique (DDPM) :</strong>
On entraîne un réseau <span class="math notranslate nohighlight">\(\epsilon_\theta(x_t, t, s)\)</span> à débruiter une commande bruitée <span class="math notranslate nohighlight">\(x_t\)</span> à l’étape <span class="math notranslate nohighlight">\(t\)</span>.</p>
<div class="math notranslate nohighlight">
\[\mathcal{L}_{Diff} = \mathbb{E}_{x_0, \epsilon, t} \left[ || \epsilon - \epsilon_\theta(\sqrt{\bar{\alpha}_t} x_0 + \sqrt{1-\bar{\alpha}_t} \epsilon, t, s) ||^2 \right]\]</div>
<p>Où <span class="math notranslate nohighlight">\(\bar{\alpha}_t\)</span> suit un schedule cosine ou linéaire défini.</p>
<p><strong>Inférence (Guided Projection) :</strong>
Pour projeter une commande cible <span class="math notranslate nohighlight">\(x_{target}\)</span> :</p>
<ol class="arabic simple">
<li><p><strong>Initialisation</strong> : On part d’une version bruitée de la cible : <span class="math notranslate nohighlight">\(x_T \sim \mathcal{N}(x_{target}, \sigma^2 I)\)</span>.</p></li>
<li><p><strong>Processus Inverse</strong> : On itère pour <span class="math notranslate nohighlight">\(t = T \dots 1\)</span> :
<span class="math notranslate nohighlight">\(x_{t-1} = \frac{1}{\sqrt{\alpha_t}} (x_t - \frac{1-\alpha_t}{\sqrt{1-\bar{\alpha}_t}} \epsilon_\theta(x_t, t, s))\)</span></p></li>
<li><p><strong>Résultat</strong> : La commande finale <span class="math notranslate nohighlight">\(x_0\)</span> a “migré” sur le manifold des données valides.</p></li>
</ol>
<ul class="simple">
<li><p><strong>Verdict</strong> : <strong>Optimal</strong>. Offre un “freinage adaptatif” et respecte les corrélations complexes entre retour et horizon.</p></li>
</ul>
</section>
</section>
<section id="bibliographie-liens-utiles">
<h2><a class="toc-backref" href="#id10" role="doc-backlink">6. Bibliographie &amp; Liens Utiles</a><a class="headerlink" href="#bibliographie-liens-utiles" title="Link to this heading">¶</a></h2>
<p>Voici les publications fondamentales citées dans ce document :</p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/abs/1912.02877">Reinforcement Learning Upside Down: Don’t Predict Rewards – Just Map Them to Actions</a>
<em>J. Schmidhuber (2019)</em> - Le papier fondateur de l’UDRL.</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2006.04779">Conservative Q-Learning for Offline Reinforcement Learning</a>
<em>Kumar et al. (NeurIPS 2020)</em> - La méthode CQL (Value Regularization).</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2110.06169">Offline Reinforcement Learning with Implicit Q-Learning</a>
<em>Kostrikov et al. (ICLR 2022)</em> - La méthode IQL (Expectile Regression).</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2106.06860">A Minimalist Approach to Offline Reinforcement Learning</a>
<em>Fujimoto &amp; Gu (NeurIPS 2021)</em> - La méthode TD3+BC (Policy Constraint).</p></li>
<li><p><a class="reference external" href="https://arxiv.org/abs/2006.11239">Denoising Diffusion Probabilistic Models</a>
<em>Ho et al. (NeurIPS 2020)</em> - Les fondements des modèles de diffusion.</p></li>
</ul>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="Footer">
        <a href="usage.html" class="btn btn-neutral float-left" title="Usage Guide" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> Previous</a>
        <a href="methodology.html" class="btn btn-neutral float-right" title="Méthodologie Algorithmique" accesskey="n" rel="next">Next <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; Copyright 2025, User.</p>
  </div>

  Built with <a href="https://www.sphinx-doc.org/">Sphinx</a> using a
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">theme</a>
    provided by <a href="https://readthedocs.org">Read the Docs</a>.
   

</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script> 

</body>
</html>